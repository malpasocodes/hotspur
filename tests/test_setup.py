#!/usr/bin/env python3
"""
Test script to verify the Shakespeare fine-tuning setup works correctly.
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

def test_setup():
    """Test that all components are working."""
    print("ğŸ­ Testing Shakespeare LLM Setup")
    print("="*50)
    
    # Check if Shakespeare data exists
    shakespeare_file = "data/processed/shakespeare_only.txt"
    if os.path.exists(shakespeare_file):
        with open(shakespeare_file, 'r', encoding='utf-8') as f:
            content = f.read()
        print(f"âœ… Shakespeare data found: {len(content):,} characters")
        print(f"ğŸ“– First 100 chars: {content[:100]}...")
    else:
        print(f"âŒ Shakespeare data not found at {shakespeare_file}")
        return False
    
    # Test PyTorch
    print(f"ğŸ”¥ PyTorch version: {torch.__version__}")
    print(f"ğŸ–¥ï¸  CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    # Test Transformers
    try:
        print("ğŸ¤— Testing Hugging Face Transformers...")
        tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
        model = AutoModelForCausalLM.from_pretrained("distilgpt2")
        
        # Test tokenization
        test_text = "To be or not to be, that is the question"
        tokens = tokenizer.encode(test_text)
        print(f"âœ… Tokenization works: '{test_text}' -> {len(tokens)} tokens")
        
        # Test model forward pass
        with torch.no_grad():
            input_ids = torch.tensor([tokens])
            outputs = model(input_ids)
            logits = outputs.logits
        print(f"âœ… Model forward pass works: output shape {logits.shape}")
        
        print("âœ… All tests passed! Ready for Shakespeare fine-tuning!")
        return True
        
    except Exception as e:
        print(f"âŒ Error testing transformers: {e}")
        return False

def quick_generation_test():
    """Test basic text generation."""
    print("\nğŸª Testing text generation...")
    
    try:
        from transformers import pipeline
        
        generator = pipeline("text-generation", model="distilgpt2", max_length=50)
        result = generator("To be or not to be", do_sample=True, temperature=0.8)
        
        print("âœ… Generation test successful!")
        print(f"ğŸ“ Generated: {result[0]['generated_text']}")
        return True
        
    except Exception as e:
        print(f"âŒ Generation test failed: {e}")
        return False

if __name__ == "__main__":
    success = test_setup()
    if success:
        quick_generation_test()
        print("\nğŸ‰ Setup is working perfectly!")
        print("ğŸš€ You can now run:")
        print("   python quick_start.py train")
        print("   python quick_start.py generate 'your prompt'")
    else:
        print("\nâŒ Setup incomplete. Please check the errors above.")
